"""
ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ
- ì•ˆì „ ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
- ëª¨ë¸ ì €ì¥ ë° í‰ê°€
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib


# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
MODEL_DIR = PROJECT_ROOT / "models"

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
MODEL_DIR.mkdir(exist_ok=True)


def load_training_data(filepath: str) -> pd.DataFrame:
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    df = pd.read_csv(filepath)
    print(f"âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ: {len(df)} í–‰")
    return df


def prepare_features(df: pd.DataFrame) -> tuple:
    """
    í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
    
    Returns:
        X: í”¼ì²˜ DataFrame
        y: íƒ€ê²Ÿ Series
    """
    feature_columns = [
        "dist_to_streetlight",
        "dist_to_police",
        "dist_to_main_road",
        "streetlight_count_100m"
    ]
    
    target_column = "safety_score"
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]
    if missing_cols:
        raise ValueError(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_cols}")
    
    X = df[feature_columns]
    y = df[target_column]
    
    return X, y


def train_model(X: pd.DataFrame, y: pd.Series, model_type: str = "random_forest") -> dict:
    """
    ëª¨ë¸ í•™ìŠµ
    
    Args:
        X: í”¼ì²˜ ë°ì´í„°
        y: íƒ€ê²Ÿ ë°ì´í„°
        model_type: 'random_forest' ë˜ëŠ” 'gradient_boosting'
    
    Returns:
        í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ë©”íŠ¸ë¦­)
    """
    print(f"\nğŸ”„ {model_type} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
    if model_type == "random_forest":
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
    elif model_type == "gradient_boosting":
        model = GradientBoostingRegressor(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    model.fit(X_train_scaled, y_train)
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    y_pred = model.predict(X_test_scaled)
    
    metrics = {
        "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),
        "mae": mean_absolute_error(y_test, y_pred),
        "r2": r2_score(y_test, y_pred)
    }
    
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
    print(f"   - RMSE: {metrics['rmse']:.4f}")
    print(f"   - MAE: {metrics['mae']:.4f}")
    print(f"   - RÂ²: {metrics['r2']:.4f}")
    
    # í”¼ì²˜ ì¤‘ìš”ë„ ì¶œë ¥
    if hasattr(model, "feature_importances_"):
        print(f"\nğŸ“ˆ í”¼ì²˜ ì¤‘ìš”ë„:")
        for feat, imp in zip(X.columns, model.feature_importances_):
            print(f"   - {feat}: {imp:.4f}")
    
    return {
        "model": model,
        "scaler": scaler,
        "metrics": metrics,
        "feature_names": list(X.columns)
    }


def save_model(result: dict, model_name: str = "safety_model"):
    """ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥"""
    model_path = MODEL_DIR / f"{model_name}.joblib"
    scaler_path = MODEL_DIR / f"{model_name}_scaler.joblib"
    
    joblib.dump(result["model"], model_path)
    joblib.dump(result["scaler"], scaler_path)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "feature_names": result["feature_names"],
        "metrics": result["metrics"]
    }
    metadata_path = MODEL_DIR / f"{model_name}_metadata.joblib"
    joblib.dump(metadata, metadata_path)
    
    print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
    print(f"   - ëª¨ë¸: {model_path}")
    print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
    print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_path}")


def main():
    """ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    print("=" * 50)
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # í•™ìŠµ ë°ì´í„° ê²½ë¡œ (ì „ì²˜ë¦¬ í›„ ìƒì„±ëœ í”¼ì²˜ ë°ì´í„°)
    training_data_path = DATA_DIR / "processed" / "training_features.csv"
    
    if not training_data_path.exists():
        print(f"âŒ í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {training_data_path}")
        print("ğŸ’¡ ë¨¼ì € preprocessing.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í”¼ì²˜ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        
        # ë°ëª¨ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        print("\nğŸ“ ë°ëª¨ìš© ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        create_sample_training_data()
        
    # ë°ì´í„° ë¡œë“œ
    df = load_training_data(training_data_path)
    
    # í”¼ì²˜ ì¤€ë¹„
    X, y = prepare_features(df)
    
    # ëª¨ë¸ í•™ìŠµ
    result = train_model(X, y, model_type="random_forest")
    
    # ëª¨ë¸ ì €ì¥
    save_model(result, "safety_model")
    
    print("\n" + "=" * 50)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 50)


def create_sample_training_data():
    """ë°ëª¨ìš© ìƒ˜í”Œ í•™ìŠµ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    n_samples = 1000
    
    # ìƒ˜í”Œ í”¼ì²˜ ìƒì„±
    data = {
        "latitude": np.random.uniform(37.4, 37.6, n_samples),
        "longitude": np.random.uniform(126.8, 127.1, n_samples),
        "dist_to_streetlight": np.random.exponential(30, n_samples),
        "dist_to_police": np.random.exponential(500, n_samples),
        "dist_to_main_road": np.random.exponential(100, n_samples),
        "streetlight_count_100m": np.random.poisson(3, n_samples)
    }
    
    # ì•ˆì „ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ê³µì‹)
    data["safety_score"] = (
        100 
        - np.minimum(20, data["dist_to_streetlight"] / 5)
        - np.minimum(30, data["dist_to_police"] / 50)
        - np.minimum(20, data["dist_to_main_road"] / 10)
        + np.minimum(15, data["streetlight_count_100m"] * 3)
        + np.random.normal(0, 5, n_samples)  # ë…¸ì´ì¦ˆ ì¶”ê°€
    )
    data["safety_score"] = np.clip(data["safety_score"], 0, 100)
    
    df = pd.DataFrame(data)
    
    # ì €ì¥
    output_dir = DATA_DIR / "processed"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "training_features.csv"
    df.to_csv(output_path, index=False)
    
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_path}")


if __name__ == "__main__":
    main()
